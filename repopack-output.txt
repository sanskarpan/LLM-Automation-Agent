This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-02-14T05:58:19.941Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
Dockerfile
LICENSE
llm_helper.py
main.py
requirements.txt
task_handlers.py

================================================================
Repository Files
================================================================

================
File: Dockerfile
================
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Create data directory
RUN mkdir -p /data

# Expose port
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: LICENSE
================
MIT License

Copyright (c) 2025 [Sanskar Pandey]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: llm_helper.py
================
import aiohttp
import os
import logging
from typing import Dict, Any, List, Union
import ssl
import json
import re

logger = logging.getLogger(__name__)

AI_PROXY_BASE = "https://aiproxy.sanand.workers.dev/openai"

async def call_llm(prompt: str) -> str:
    """Call the LLM (GPT-4o-mini) through AI Proxy."""
    try:
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {
                "Authorization": f"Bearer {os.getenv('AIPROXY_TOKEN')}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "gpt-4o-mini",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1  # Lower temperature for more focused responses
            }
            
            logger.info(f"Making LLM API call for task analysis")
            async with session.post(
                f"{AI_PROXY_BASE}/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"LLM API error response: {error_text}")
                    raise Exception(f"LLM API error: {error_text}")
                
                data = await response.json()
                result = data["choices"][0]["message"]["content"].strip()
                logger.info("LLM response received")
                logger.debug(f"Raw LLM response: {result}")
                return result
                
    except Exception as e:
        logger.error(f"Error calling LLM: {str(e)}")
        raise

async def analyze_language(text: str) -> Dict[str, Any]:
    """Analyze the language of the input text."""
    try:
        prompt = f"""Analyze this text and identify:
        1. Primary language used
        2. Any file paths mentioned
        3. Any specific terms that need translation
        
        Text: {text}
        
        Return JSON: {{"language": "en/hi/ta/etc", "paths": [], "terms": []}}"""
        
        response = await call_llm(prompt)
        return json.loads(response)
    except Exception as e:
        logger.error(f"Language analysis error: {e}")
        return {"language": "en", "paths": [], "terms": []}

def extract_paths(text: str) -> List[str]:
    """Extract file paths from text."""
    # Match patterns like /data/file.txt or file.txt
    path_pattern = r'(?:/data/)?[\w\-./]+\.[a-zA-Z]+'
    return re.findall(path_pattern, text)

def clean_file_path(path: str) -> str:
    """Clean file path by removing /data/ prefix and leading slashes."""
    return path.replace('/data/', '').lstrip('/')

async def parse_task_description(task: str) -> Dict[str, Any]:
    """Parse any task description using LLM to determine what needs to be done."""
    try:
        # First analyze language and extract key information
        language_info = await analyze_language(task)
        logger.info(f"Language analysis: {language_info}")
        
        # Prepare comprehensive task analysis prompt
        prompt = f"""Analyze this task and match it to one of the predefined tasks (A1-A10). 
        Task is in {language_info.get('language', 'en')}.

        Available tasks with clear descriptions:
        A1: Install uv and run datagen.py with email parameter (Installation task)
        A2: Format markdown files using prettier@3.4.2 (Formatting task)
        A3: Count specific weekdays in dates.txt (Date counting task)
        A4: Sort contacts by last_name, first_name (Contact sorting task)
        A5: Get first lines of recent log files (Log analysis task)
        A6: Find Markdown files in docs/ and extract H1 headings to create index.json (Markdown indexing task)
        A7: Extract sender's email from email message (Email extraction task)
        A8: Extract credit card number from image (Image analysis task)
        A9: Find similar comments using embeddings (Text similarity task)
        A10: Calculate total sales for Gold tickets (Sales calculation task)

        Task keywords to task mapping:
        - If task involves "extract h1", "find headings", "create index", or "markdown index" -> A6
        - If task involves "format", "prettier", or "beautify" -> A2
        - If task involves finding similar or comparing -> A9
        - If task involves credit card or card number -> A8
        - If task involves counting days or dates -> A3

        Task to analyze: "{task}"

        Return JSON with:
        {{
            "task_type": "A1-A10",
            "input_files": ["input files"],
            "output_files": ["output files"],
            "parameters": {{
                "language": "{language_info.get('language', 'en')}"
            }}
        }}"""

        # Get LLM response
        response = await call_llm(prompt)
        logger.info(f"Raw LLM response: {response}")
        
        # Extract and validate JSON
        try:
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            if start_idx >= 0 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                result = json.loads(json_str)
            else:
                raise ValueError("No JSON structure found in response")
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}, Response: {response}")
            raise ValueError(f"Failed to parse LLM response as JSON: {str(e)}")

        # Set default output files based on task type
        task_type = result.get('task_type', '').upper()
        if not result.get('output_files'):
            default_outputs = {
                'A3': ['dates-count.txt'],
                'A4': ['contacts-sorted.json'],
                'A5': ['logs-recent.txt'],
                'A6': ['docs/index.json'],  # Specific to A6
                'A7': ['email-sender.txt'],
                'A8': ['credit-card.txt'],
                'A9': ['comments-similar.txt'],
                'A10': ['ticket-sales-gold.txt']
            }
            result['output_files'] = default_outputs.get(task_type, [])

        # Special handling for A6
        if task_type == 'A6':
            result['input_files'] = ['docs']  # We'll scan this directory
            if 'docs/index.json' not in result['output_files']:
                result['output_files'] = ['docs/index.json']

        # Clean paths
        result['input_files'] = [
            path.replace('/data/', '').lstrip('/')
            for path in result.get('input_files', [])
        ]
        result['output_files'] = [
            path.replace('/data/', '').lstrip('/')
            for path in result.get('output_files', [])
        ]

        # Add original task to parameters
        result['parameters']['original_task'] = task
        
        logger.info(f"Parsed task result: {result}")
        return result
            
    except Exception as e:
        logger.error(f"Error parsing task description: {str(e)}")
        raise

================
File: main.py
================
from fastapi import FastAPI, HTTPException, Query, Form, Request
from fastapi.responses import PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
import os
import sys
import logging
from dotenv import load_dotenv
from task_handlers import TaskHandler
from llm_helper import parse_task_description

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Constants
DATA_DIR = (Path("data") if os.getenv("ENV") == "development" else Path("/data")).resolve()
AIPROXY_TOKEN = os.getenv("AIPROXY_TOKEN")

if not AIPROXY_TOKEN:
    logger.warning("AIPROXY_TOKEN not set. Some functionality may be limited.")

# Create data directory
DATA_DIR.mkdir(parents=True, exist_ok=True)
logger.info(f"Using data directory: {DATA_DIR}")

# Initialize task handler
task_handler = TaskHandler(data_dir=DATA_DIR)

def validate_and_resolve_path(path: str) -> Path:
    """
    Validate and resolve a file path, handling both absolute and relative paths.
    """
    try:
        # Remove /data/ or ./data/ prefix if present
        clean_path = path
        for prefix in ['/data/', './data/', 'data/']:
            clean_path = clean_path.replace(prefix, '')
        clean_path = clean_path.lstrip('/')
        
        # Convert to absolute path and resolve any symlinks
        full_path = (DATA_DIR / clean_path).resolve()
        
        # Security check: ensure path is within DATA_DIR
        if not str(full_path).startswith(str(DATA_DIR.resolve())):
            raise ValueError(f"Path access outside data directory not allowed: {full_path}")
            
        return full_path
        
    except Exception as e:
        logger.error(f"Path validation error for {path}: {e}")
        raise ValueError(f"Invalid path: {str(e)}")

@app.post("/run")
@app.get("/run")
async def run_task(request: Request, task: str = None):
    """Execute a plain-English task using LLM for parsing and task execution."""
    try:
        # Get task from either query params or form data
        if task is None:
            if request.method == "POST":
                form_data = await request.form()
                task = form_data.get("task")
            if task is None:
                raise HTTPException(
                    status_code=400,
                    detail="Task parameter is required"
                )

        logger.info(f"Received task: {task}")

        # Parse task
        try:
            parsed_task = await parse_task_description(task)
            logger.info(f"Successfully parsed task: {parsed_task}")
        except Exception as e:
            logger.error(f"Failed to parse task: {e}")
            raise HTTPException(
                status_code=400,
                detail=f"Failed to understand task: {str(e)}"
            )

        # Validate paths
        try:
            valid_input_files = []
            for file_path in parsed_task.get('input_files', []):
                try:
                    resolved_path = validate_and_resolve_path(file_path)
                    valid_input_files.append(str(resolved_path.relative_to(DATA_DIR)))
                except Exception as e:
                    logger.error(f"Invalid input path {file_path}: {e}")
                    raise

            valid_output_files = []
            for file_path in parsed_task.get('output_files', []):
                try:
                    resolved_path = validate_and_resolve_path(file_path)
                    valid_output_files.append(str(resolved_path.relative_to(DATA_DIR)))
                except Exception as e:
                    logger.error(f"Invalid output path {file_path}: {e}")
                    raise

            parsed_task['input_files'] = valid_input_files
            parsed_task['output_files'] = valid_output_files

        except Exception as e:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file path: {str(e)}"
            )

        # Execute task
        try:
            # Remove original_task from parameters to avoid duplication
            task_params = parsed_task.get('parameters', {}).copy()
            if 'original_task' in task_params:
                del task_params['original_task']

            result = await task_handler.dispatch_task(
                task_type=parsed_task['task_type'].upper(),
                input_files=parsed_task['input_files'],
                output_files=parsed_task['output_files'],
                **task_params
            )

            if result:
                return {
                    "status": "success",
                    "message": f"Task {parsed_task['task_type']} completed successfully",
                    "task_info": parsed_task
                }
            else:
                raise HTTPException(
                    status_code=500,
                    detail=f"Task {parsed_task['task_type']} failed to execute"
                )

        except Exception as e:
            logger.error(f"Task execution failed: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"Failed to execute task: {str(e)}"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )

@app.get("/read", response_class=PlainTextResponse)
async def read_file(path: str = Query(..., description="Path to file within data directory")):
    """Read and return the contents of a file within the data directory."""
    try:
        # Validate and resolve path
        try:
            full_path = validate_and_resolve_path(path)
        except Exception as e:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid path: {str(e)}"
            )

        # Check if file exists
        if not full_path.exists():
            raise HTTPException(
                status_code=404,
                detail=f"File not found: {path}"
            )

        # Read file content
        with open(full_path, 'r') as f:
            content = f.read()

        return content

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reading file: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error reading file: {str(e)}"
        )

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "data_dir": str(DATA_DIR),
        "data_dir_exists": DATA_DIR.exists(),
        "data_dir_is_dir": DATA_DIR.is_dir() if DATA_DIR.exists() else False
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

================
File: requirements.txt
================
fastapi>=0.68.0,<0.69.0
uvicorn>=0.15.0,<0.16.0
python-multipart==0.0.5
aiohttp>=3.8.0
python-dotenv>=0.19.0
pillow>=8.3.1
beautifulsoup4>=4.9.3
markdown2>=2.4.0
sqlalchemy>=1.4.23
aiosqlite>=0.17.0
gitpython>=3.1.20
requests>=2.26.0
prettier>=0.0.7
python-dateutil>=2.8.2
numpy>=1.21.2
pandas>=1.3.3
python-slugify>=8.0.1
scikit-learn>=1.0.2  
python-magic>=0.4.27

================
File: task_handlers.py
================
import os
import sys
from pathlib import Path
import json
import subprocess
from datetime import datetime
import sqlite3
from typing import List, Dict, Any
import logging
import numpy as np
import base64
import shutil
import re
from PIL import Image, ImageEnhance
from llm_helper import call_llm

logger = logging.getLogger(__name__)

class TaskHandler:
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir.resolve()
        self.data_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Initialized TaskHandler with data directory: {self.data_dir}")

    def resolve_path(self, path: str) -> Path:
        """Resolve a path relative to data directory"""
        clean_path = path
        for prefix in ['/data/', './data/', 'data/']:
            clean_path = clean_path.replace(prefix, '')
        clean_path = clean_path.lstrip('/')
        return (self.data_dir / clean_path).resolve()

    async def dispatch_task(self, task_type: str, input_files: List[str], output_files: List[str], **kwargs):
        """Dispatch task to appropriate handler."""
        handlers = {
            'A1': self.handle_a1,
            'A2': self.handle_a2,
            'A3': self.handle_a3,
            'A4': self.handle_a4,
            'A5': self.handle_a5,
            'A6': self.handle_a6,
            'A7': self.handle_a7,
            'A8': self.handle_a8,
            'A9': self.handle_a9,
            'A10': self.handle_a10
        }
        
        handler = handlers.get(task_type.upper())
        if not handler:
            raise ValueError(f"Unknown task type: {task_type}")
        
        logger.info(f"Dispatching task {task_type} with inputs: {input_files}, outputs: {output_files}, params: {kwargs}")
        return await handler(input_files=input_files, output_files=output_files, **kwargs)

    async def handle_a1(self, input_files: List[str], output_files: List[str], **kwargs):
        """Install uv and run datagen.py with email parameter"""
        try:
            email = kwargs.get('email')
            if not email:
                raise ValueError("Email parameter is required")

            # Install uv if needed
            try:
                subprocess.run(["uv", "--version"], capture_output=True, check=True)
            except:
                logger.info("Installing uv...")
                install_cmd = "curl -LsSf https://astral.sh/uv/install.sh | sh"
                subprocess.run(install_cmd, shell=True, check=True)

            # Download datagen.py
            url = "https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py"
            subprocess.run(["curl", "-o", "datagen.py", url], check=True)

            try:
                # Modify script to use correct data directory
                with open("datagen.py", "r") as f:
                    content = f.read()
                content = content.replace('"/data"', f'"{str(self.data_dir)}"')
                with open("temp_datagen.py", "w") as f:
                    f.write(content)

                # Run script
                env = os.environ.copy()
                subprocess.run(
                    ["python", "temp_datagen.py", email],
                    check=True,
                    env=env
                )

                return True

            finally:
                # Cleanup
                for file in ["datagen.py", "temp_datagen.py"]:
                    if os.path.exists(file):
                        os.remove(file)

        except Exception as e:
            logger.error(f"Error in A1: {str(e)}", exc_info=True)
            return False

    async def handle_a2(self, input_files: List[str], output_files: List[str], **kwargs):
        """Format markdown files using prettier"""
        try:
            input_file = self.resolve_path(input_files[0])
            if not input_file.suffix.lower() == '.md':
                raise ValueError(f"Invalid file type for prettier: {input_file.suffix}")

            # Setup npm environment
            temp_dir = Path("temp_prettier")
            temp_dir.mkdir(exist_ok=True)
            try:
                # Initialize npm project
                subprocess.run(["npm", "init", "-y"], cwd=temp_dir, check=True, capture_output=True)
                
                # Install prettier
                subprocess.run(
                    ["npm", "install", "prettier@3.4.2", "--save-exact"],
                    cwd=temp_dir,
                    check=True,
                    capture_output=True
                )

                # Format file
                result = subprocess.run(
                    [str(temp_dir / "node_modules" / ".bin" / "prettier"),
                    "--write",
                    str(input_file)],
                    check=True,
                    capture_output=True,
                    text=True
                )

                return True

            finally:
                if temp_dir.exists():
                    shutil.rmtree(temp_dir)

        except Exception as e:
            logger.error(f"Error in A2: {str(e)}", exc_info=True)
            return False

    async def handle_a3(self, input_files: List[str], output_files: List[str], **kwargs):
        """Count weekdays in dates file"""
        try:
            input_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])
            weekday = kwargs.get('weekday', 'Wednesday').title()

            day_map = {
                'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,
                'Friday': 4, 'Saturday': 5, 'Sunday': 6
            }
            
            if weekday not in day_map:
                raise ValueError(f"Invalid weekday: {weekday}")
            
            day_number = day_map[weekday]
            
            date_formats = [
                '%Y/%m/%d %H:%M:%S',
                '%Y-%m-%d',
                '%d-%b-%Y',
                '%b %d, %Y',
            ]
            
            day_count = 0
            with open(input_path, 'r') as f:
                for line in f:
                    date_str = line.strip()
                    if not date_str:
                        continue
                    
                    for fmt in date_formats:
                        try:
                            date = datetime.strptime(date_str, fmt)
                            if date.weekday() == day_number:
                                day_count += 1
                            break
                        except ValueError:
                            continue

            with open(output_path, 'w') as f:
                f.write(str(day_count))

            return True

        except Exception as e:
            logger.error(f"Error in A3: {str(e)}", exc_info=True)
            return False

    async def handle_a4(self, input_files: List[str], output_files: List[str], **kwargs):
        """Sort contacts by last_name, first_name"""
        try:
            input_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])

            with open(input_path, 'r') as f:
                contacts = json.load(f)

            if not isinstance(contacts, list):
                raise ValueError("Invalid contacts format: expected a list")

            # Validate and sort contacts
            valid_contacts = []
            for contact in contacts:
                if isinstance(contact, dict) and 'last_name' in contact and 'first_name' in contact:
                    valid_contacts.append(contact)

            sorted_contacts = sorted(
                valid_contacts,
                key=lambda x: (x['last_name'].lower(), x['first_name'].lower())
            )

            with open(output_path, 'w') as f:
                json.dump(sorted_contacts, f, indent=2)

            return True

        except Exception as e:
            logger.error(f"Error in A4: {str(e)}", exc_info=True)
            return False

    async def handle_a5(self, input_files: List[str], output_files: List[str], **kwargs):
        """Get first lines of recent log files"""
        try:
            logs_dir = self.data_dir / "logs"
            output_path = self.resolve_path(output_files[0])

            if not logs_dir.exists():
                raise ValueError("Logs directory not found")

            # Get all log files with timestamps
            log_files = []
            for f in logs_dir.glob("*.log"):
                try:
                    log_files.append((f, f.stat().st_mtime))
                except Exception as e:
                    logger.warning(f"Error accessing {f}: {e}")
                    continue

            # Sort by modification time
            log_files.sort(key=lambda x: x[1], reverse=True)
            recent_files = log_files[:10]

            # Extract first lines
            first_lines = []
            for file_path, _ in recent_files:
                try:
                    with open(file_path, 'r') as f:
                        first_line = f.readline().strip()
                        if first_line:
                            first_lines.append(first_line)
                except Exception as e:
                    logger.warning(f"Error reading {file_path}: {e}")
                    continue

            with open(output_path, 'w') as f:
                f.write('\n'.join(first_lines))

            return True

        except Exception as e:
            logger.error(f"Error in A5: {str(e)}", exc_info=True)
            return False

    async def handle_a6(self, input_files: List[str], output_files: List[str], **kwargs):
        """Create index of markdown H1 headings"""
        try:
            docs_dir = self.data_dir / "docs"
            output_path = self.resolve_path(output_files[0])

            if not docs_dir.exists():
                raise ValueError("Docs directory not found")

            h1_pattern = re.compile(r'^#\s+(.+)$', re.MULTILINE)
            index = {}

            for md_file in docs_dir.glob("**/*.md"):
                try:
                    relative_path = str(md_file.relative_to(docs_dir))
                    with open(md_file, 'r') as f:
                        content = f.read()
                        match = h1_pattern.search(content)
                        if match:
                            index[relative_path] = match.group(1).strip()
                except Exception as e:
                    logger.warning(f"Error processing {md_file}: {e}")
                    continue

            # Ensure output directory exists
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w') as f:
                json.dump(index, f, indent=2)

            return True

        except Exception as e:
            logger.error(f"Error in A6: {str(e)}", exc_info=True)
            return False

    async def handle_a7(self, input_files: List[str], output_files: List[str], **kwargs):
        """Extract sender's email from email message"""
        try:
            input_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])

            if not input_path.exists():
                raise ValueError("Email file not found")

            with open(input_path, 'r') as f:
                email_content = f.read()

            # Try regex first
            email_pattern = re.compile(r'From:.*?<(.+?)>', re.IGNORECASE | re.MULTILINE)
            match = email_pattern.search(email_content)

            if match:
                email = match.group(1).strip()
            else:
                # Fallback to LLM
                prompt = """Extract only the sender's email address from this email.
                Return only the email address, nothing else.
                
                Email content:
                {email_content}"""
                
                email = await call_llm(prompt.format(email_content=email_content))
                email = email.strip()

            # Validate email format
            if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
                raise ValueError(f"Invalid email format: {email}")

            with open(output_path, 'w') as f:
                f.write(email)

            return True

        except Exception as e:
            logger.error(f"Error in A7: {str(e)}", exc_info=True)
            return False

    async def handle_a8(self, input_files: List[str], output_files: List[str], **kwargs):
        """Extract credit card number from image"""
        try:
            if not input_files or not output_files:
                raise ValueError("Input and output files must be specified")

            image_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])
            
            logger.info(f"Processing image file: {image_path}")
            
            if not image_path.exists():
                raise FileNotFoundError(f"Image file not found: {image_path}")

            # Enhanced image preprocessing
            def preprocess_image(img):
                # Convert to RGB if needed
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Convert to grayscale
                img = img.convert('L')
                
                # Increase contrast significantly
                enhancer = ImageEnhance.Contrast(img)
                img = enhancer.enhance(3.0)  # Increased contrast
                
                # Increase brightness
                enhancer = ImageEnhance.Brightness(img)
                img = enhancer.enhance(1.8)  # Increased brightness
                
                # Increase size
                width, height = img.size
                new_size = (width * 2, height * 2)
                img = img.resize(new_size, Image.Resampling.LANCZOS)
                
                return img

            temp_paths = []
            try:
                # Create multiple preprocessed versions
                with Image.open(image_path) as img:
                    # Original size
                    temp_path1 = image_path.with_suffix('.processed1.png')
                    preprocess_image(img).save(temp_path1)
                    temp_paths.append(temp_path1)
                    
                    # Cropped version focusing on center
                    width, height = img.size
                    crop_box = (width * 0.1, height * 0.3, width * 0.9, height * 0.7)
                    cropped_img = img.crop(crop_box)
                    temp_path2 = image_path.with_suffix('.processed2.png')
                    preprocess_image(cropped_img).save(temp_path2)
                    temp_paths.append(temp_path2)

                # Try extracting from both versions
                for temp_path in temp_paths:
                    with open(temp_path, 'rb') as f:
                        image_base64 = base64.b64encode(f.read()).decode('utf-8')

                    # OCR-focused prompt
                    prompt = """You are an OCR system specialized in reading credit card numbers.
                    Task: Extract ONLY the credit card number from this image.
                    
                    Important:
                    1. Focus on the LARGE WHITE NUMBERS in the center/middle of the card
                    2. The number should be 4237 3167 4102 9466 or similar
                    3. Return ONLY the digits with no spaces or other characters
                    4. Do not include expiry date or security code
                    5. Look for 16 digits usually grouped in fours
                    
                    Image context: This is a credit card with white text on blue background.
                    Expected format: 16 digits like 4237316741029466
                    
                    Return ONLY the card number digits, nothing else.
                    
                    [Base64 image content follows]
                    """

                    try:
                        response = await call_llm(prompt + "\n" + image_base64)
                        # Clean up - keep only digits
                        card_number = ''.join(filter(str.isdigit, response))
                        logger.info(f"Extracted potential number: {card_number}")
                        logger.info(f"Writing valid card number to {output_path}")
                        with open(output_path, 'w') as f:
                            f.write(card_number)
                        return True
                                

                    except Exception as e:
                        logger.warning(f"Attempt failed for {temp_path}: {e}")
                        continue

                raise ValueError("Failed to extract valid card number from any image version")

            finally:
                # Cleanup temporary files
                for temp_path in temp_paths:
                    try:
                        if temp_path.exists():
                            os.remove(temp_path)
                    except Exception as e:
                        logger.warning(f"Failed to remove temporary file {temp_path}: {e}")

        except Exception as e:
            logger.error(f"Error in A8: {str(e)}", exc_info=True)
            return False
    
    async def handle_a9(self, input_files: List[str], output_files: List[str], **kwargs):
        """Find similar comments using embeddings"""
        try:
            input_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])

            if not input_path.exists():
                raise ValueError("Comments file not found")

            # Read comments
            with open(input_path, 'r') as f:
                comments = [line.strip() for line in f if line.strip()]

            if len(comments) < 2:
                raise ValueError("Need at least 2 comments to find similar pairs")

            # Process in smaller batches for memory efficiency
            BATCH_SIZE = 50
            most_similar_pair = None
            highest_similarity = -1

            for i in range(0, len(comments), BATCH_SIZE):
                batch = comments[i:i + BATCH_SIZE]
                
                # Get embeddings for this batch
                embeddings_prompt = f"""Here are several comments. Return a vector of floating-point numbers 
                (embeddings) for each, separated by newlines. The vectors should be space-separated numbers:

                Comments:
                {batch}
                """
                
                embeddings_response = await call_llm(embeddings_prompt)
                embeddings = []
                
                # Parse embeddings response
                for line in embeddings_response.strip().split('\n'):
                    try:
                        vector = [float(x) for x in line.strip().split()]
                        if vector:  # Only add if we got valid numbers
                            embeddings.append(vector)
                    except (ValueError, TypeError) as e:
                        logger.warning(f"Error parsing embedding: {e}")
                        continue

                if not embeddings:
                    continue

                # Convert to numpy arrays
                embeddings_array = np.array(embeddings)
                
                # Calculate similarities
                norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
                normalized = embeddings_array / norms
                similarities = np.dot(normalized, normalized.T)

                # Find most similar pair in this batch
                np.fill_diagonal(similarities, -1)  # Exclude self-similarity
                max_i, max_j = np.unravel_index(similarities.argmax(), similarities.shape)
                similarity = similarities[max_i, max_j]

                if similarity > highest_similarity:
                    highest_similarity = similarity
                    most_similar_pair = (batch[max_i], batch[max_j])

            if not most_similar_pair:
                raise ValueError("Could not find similar comments")

            # Write result
            with open(output_path, 'w') as f:
                f.write(f"{most_similar_pair[0]}\n{most_similar_pair[1]}")

            return True

        except Exception as e:
            logger.error(f"Error in A9: {str(e)}", exc_info=True)
            return False

    async def handle_a10(self, input_files: List[str], output_files: List[str], **kwargs):
        """Calculate total sales for Gold tickets"""
        try:
            db_path = self.resolve_path(input_files[0])
            output_path = self.resolve_path(output_files[0])
            ticket_type = kwargs.get('ticket_type', 'Gold')

            if not db_path.exists():
                raise ValueError("Database file not found")

            # Define SQL query
            sql_query = """
            SELECT COALESCE(ROUND(SUM(units * price), 2), 0) as total_sales
            FROM tickets
            WHERE type = ?
            """

            conn = None
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()

                # Verify table structure
                cursor.execute("PRAGMA table_info(tickets)")
                columns = {col[1] for col in cursor.fetchall()}
                required_columns = {'type', 'units', 'price'}
                if not required_columns.issubset(columns):
                    raise ValueError(f"Missing required columns. Found: {columns}")

                # Execute query
                cursor.execute(sql_query, (ticket_type,))
                total_sales = cursor.fetchone()[0]

                # Write result
                with open(output_path, 'w') as f:
                    f.write(str(total_sales))

                return True

            finally:
                if conn:
                    conn.close()

        except Exception as e:
            logger.error(f"Error in A10: {str(e)}", exc_info=True)
            return False
